{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_processes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1263f9dcb731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# local modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_processes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m##########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_processes'"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "# code to generate and track particles with a netCDF file\n",
    "from parcels import FieldSet, ParticleSet, Variable, JITParticle, AdvectionRK4, plotTrajectoriesFile\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# local modules\n",
    "from data_processes import *\n",
    "\n",
    "def oceantime_2_dt(frame_time):\n",
    "    \"\"\"\n",
    "    Datetime is in local timezone (but not timezone aware)\n",
    "    \"\"\"\n",
    "    dtcon_days = frame_time\n",
    "    dtcon_start = datetime(1990,1,1)\n",
    "    dtcon_delta = timedelta(dtcon_days/24/60/60)\n",
    "    dtcon_offset = dtcon_start + dtcon_delta\n",
    "\n",
    "    return dtcon_offset\n",
    "\n",
    "##########\n",
    "# Config #\n",
    "##########\n",
    "ROMS_directory = \"/Users/lachlanphillips/PhD_Large_Data/ROMS/testing/\"\n",
    "\n",
    "#######################\n",
    "# Read in NetCDF data #\n",
    "#######################\n",
    "# get file list\n",
    "file_ls = [f for f in listdir(ROMS_directory) if isfile(join(ROMS_directory, f))]\n",
    "file_ls = list(filter(lambda x:'.nc' in x, file_ls))\n",
    "file_ls = sorted(file_ls)\n",
    "\n",
    "# obtain dimension information\n",
    "# load a file\n",
    "nc_file = ROMS_directory + file_ls[0]\n",
    "fh = Dataset(nc_file, mode='r')\n",
    "\n",
    "\"\"\"\n",
    "I had a problem when I recived my orginal ROMS nc files where \n",
    "the U and V xi dimension were cut off by 1 for V (165 vs 166). In \n",
    "case this occurs again I will reshape the dimensions to the minimum\n",
    "value (i.e. trim the array from 166 to 165 along xi).\n",
    "\"\"\"\n",
    "# get field set array dimensions\n",
    "lon_rho = fh.variables['lon_rho'][:,:]\n",
    "lat_rho = fh.variables['lat_rho'][:,:]\n",
    "# check dimensions agree for field set and if they don't use the\n",
    "# the smallest dimensions\n",
    "dims = [lon_rho.shape, lat_rho.shape]\n",
    "dims_min = (min([x[0] for x in dims]) , min([x[1] for x in dims]))\n",
    "\n",
    "\"\"\"\n",
    "# NOTE\n",
    "I have used lon and lat rho values but V and U actually map to eta_u\n",
    "and xi_u. But I'm getting odd results pulling those corrdinates. \n",
    "Near enough is good enough I think and rho coordinated should be fine\n",
    "to use for now unless I hear otherwise.\n",
    "Note that eta is Y (kinda lat) and xi is x (kinda lon).. kinda\n",
    "\"\"\"\n",
    "\n",
    "# Loop through files and construct 3 dimension array from surface \n",
    "# layer spatial dimensions and time.\n",
    "# empty arrays to store files\n",
    "uarray = [np.nan]*len(file_ls)\n",
    "varray = [np.nan]*len(file_ls)\n",
    "timearray = [np.nan]*len(file_ls)\n",
    "# the main loop\n",
    "i = 0\n",
    "for file in file_ls:\n",
    "    fh = Dataset(ROMS_directory + file, mode='r')\n",
    "    uarray[i] = fh.variables['u'][:,29,0:dims_min[0],0:dims_min[1]]\n",
    "    varray[i] = fh.variables['u'][:,29,0:dims_min[0],0:dims_min[1]]\n",
    "    timearray[i] = fh.variables['ocean_time'][:]\n",
    "    i += 1\n",
    "# now we need to stack the 3 dimensional arrays together.\n",
    "u = np.vstack(uarray)\n",
    "v = np.vstack(uarray)\n",
    "time = np.concatenate(timearray)\n",
    "# convert time values to datetimes\n",
    "time = [oceantime_2_dt(t) for t in time]\n",
    "\n",
    "##################################\n",
    "# Transform to Parcels feild-set #\n",
    "##################################\n",
    "# make field-set\n",
    "data = {'U':u, 'V':v}\n",
    "dimensions = {'lon':lon_rho, 'lat':lat_rho, 'time':time}\n",
    "fieldset = FieldSet.from_data(data, dimensions, mesh='flat')\n",
    "\n",
    "# show feild\n",
    "fieldset.U.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
